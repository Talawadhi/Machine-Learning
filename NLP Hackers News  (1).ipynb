{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacker News is a social news website focusing on computer science and entrepreneurship. It has a community where users can submit articles, and other users can upvote those articles. Like other website the articles with the most upvotes make it to the front page.\n",
    "\n",
    "This data set consists of submissions users made to Hacker News from 2006 to 2015. Developer Arnaud Drizard used the Hacker News API to scrape the data, which which can be  found in one of his GitHub repositories. https://github.com/arnauddri/hn\n",
    "\n",
    "hn_stories is a 3000 rows that was sampled from the data randomly, and it has only has four columns:\n",
    "\n",
    "* submission_time - When the article was submitted\n",
    "* upvotes - The number of upvotes the article received\n",
    "* url - The base URL of the article\n",
    "* headline - The article's headline\n",
    "\n",
    "I'll be predicting the number of upvotes the articles received, based on their headlines. \n",
    "Upvotes are an indicator of popularity, I will try to discover which types of articles tend to be the most popular it this community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading the file to datframe\n",
    "articals = pd.read_csv(\"hn_stories.csv\")\n",
    "articals.columns = [\"submission_time\", \"upvotes\", \"url\", \"headline\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2999 entries, 0 to 2998\n",
      "Data columns (total 4 columns):\n",
      "submission_time    2999 non-null object\n",
      "upvotes            2999 non-null int64\n",
      "url                2810 non-null object\n",
      "headline           2989 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 93.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Exploring the data\n",
    "articals.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, we have four columns three with an object data type and one numerical.\n",
    "\n",
    "There are some missing values, so I am going to remove those rows first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "articals = articals.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Since I am going to train a linear regression model to predicts the number of upvotes a headline would receive, I will need to convert each headline to a numerical representation.\n",
    "There are several ways to do this, I will use the bag of words model where each piece of text is represented as a numerical vector.\n",
    "The first step in creating a bag of words model is tokenization. I am going to split each sentence into a list of individual words on the space character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list for the tokens\n",
    "headline_tokens = []\n",
    "\n",
    "# Looping through the dataframe to split headlines and add words as a list \n",
    "for row in articals['headline']:\n",
    "    headline_tokens.append(row.split())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Software:',\n",
       "  'Sadly',\n",
       "  'we',\n",
       "  'did',\n",
       "  'adopt',\n",
       "  'from',\n",
       "  'the',\n",
       "  'construction',\n",
       "  'analogy'],\n",
       " ['Google’s',\n",
       "  'Stock',\n",
       "  'Split',\n",
       "  'Means',\n",
       "  'More',\n",
       "  'Control',\n",
       "  'for',\n",
       "  'Larry',\n",
       "  'and',\n",
       "  'Sergey'],\n",
       " ['SSL',\n",
       "  'DOS',\n",
       "  'attack',\n",
       "  'tool',\n",
       "  'released',\n",
       "  'exploiting',\n",
       "  'negotiation',\n",
       "  'overhead'],\n",
       " ['Immutability', 'and', 'Blocks', 'Lambdas', 'and', 'Closures'],\n",
       " ['Comment', 'optimiser', 'la', 'vitesse', 'de', 'Wordpress?']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "On my next step, I am going to remove punctuation lowercas all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['software',\n",
       "  'sadly',\n",
       "  'we',\n",
       "  'did',\n",
       "  'adopt',\n",
       "  'from',\n",
       "  'the',\n",
       "  'construction',\n",
       "  'analogy'],\n",
       " ['googles',\n",
       "  'stock',\n",
       "  'split',\n",
       "  'means',\n",
       "  'more',\n",
       "  'control',\n",
       "  'for',\n",
       "  'larry',\n",
       "  'and',\n",
       "  'sergey'],\n",
       " ['ssl',\n",
       "  'dos',\n",
       "  'attack',\n",
       "  'tool',\n",
       "  'released',\n",
       "  'exploiting',\n",
       "  'negotiation',\n",
       "  'overhead'],\n",
       " ['immutability', 'and', 'blocks', 'lambdas', 'and', 'closures'],\n",
       " ['comment', 'optimiser', 'la', 'vitesse', 'de', 'wordpress']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A list of punctuation to be removed from tokens\n",
    "punctuation = [ \"/\", \"-\", \"+\", \"&\", \"(\", \")\", \",\", \":\", \";\", \".\", \"'\", '\"', \"’\", \"?\"]\n",
    "\n",
    "# New list for the processed tokens\n",
    "clean_tokens = []\n",
    "\n",
    "# A loop to go through tokens and lower case each word and remove punctuation \n",
    "for tokens in headline_tokens:\n",
    "    #list for each sentance\n",
    "    tokens_list = []\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        for p in punctuation:\n",
    "            token = token.replace(p, '')\n",
    "        tokens_list.append(token)\n",
    "    clean_tokens.append(tokens_list)\n",
    "    \n",
    "clean_tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Now I am going to create a dataframe for all the unique words to convert the sentences to their numerical representations.\n",
    "I will only keep tokens that occured more than one time, tokens that only occurred once don't add to the model's prediction power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A list for tokens that only occured once\n",
    "single_tokens = []\n",
    "# List for unique tokens\n",
    "unique_tokens = []\n",
    "\n",
    "# loop through the clean tokens list to add the unique once\n",
    "for tokens in clean_tokens:\n",
    "    for token in tokens:\n",
    "        if token not in single_tokens:\n",
    "            single_tokens.append(token)\n",
    "        elif token not in unique_tokens:\n",
    "            unique_tokens.append(token)\n",
    "\n",
    "# creating a dataframe for all unique tokens as columns names and intialize it with 0 and same size as the clean_tokens list\n",
    "tokens_df = pd.DataFrame(0, index=np.arange(len(clean_tokens)), columns = unique_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2800 entries, 0 to 2799\n",
      "Columns: 2310 entries, and to disaster\n",
      "dtypes: int64(2310)\n",
      "memory usage: 49.4 MB\n"
     ]
    }
   ],
   "source": [
    "tokens_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "I will loop through the dataframe I just created and add 1 to the tokens if it is in the list of unique tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, tokens in enumerate(clean_tokens):\n",
    "    for token in tokens:\n",
    "        if token in unique_tokens:\n",
    "            tokens_df.iloc[index][token] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>for</th>\n",
       "      <th>as</th>\n",
       "      <th>you</th>\n",
       "      <th>is</th>\n",
       "      <th>the</th>\n",
       "      <th>split</th>\n",
       "      <th>good</th>\n",
       "      <th>how</th>\n",
       "      <th>what</th>\n",
       "      <th></th>\n",
       "      <th>of</th>\n",
       "      <th>de</th>\n",
       "      <th>in</th>\n",
       "      <th>a</th>\n",
       "      <th>with</th>\n",
       "      <th>amazon</th>\n",
       "      <th>cloud</th>\n",
       "      <th>at</th>\n",
       "      <th>google</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  for  as  you  is  the  split  good  how  what     of  de  in  a  with  \\\n",
       "0    0    0   0    0   0    1      0     0    0     0  0   0   0   0  0     0   \n",
       "1    1    1   0    0   0    0      1     0    0     0  0   0   0   0  0     0   \n",
       "2    0    0   0    0   0    0      0     0    0     0  0   0   0   0  0     0   \n",
       "3    2    0   0    0   0    0      0     0    0     0  0   0   0   0  0     0   \n",
       "4    0    0   0    0   0    0      0     0    0     0  0   0   1   0  0     0   \n",
       "5    0    1   2    2   1    0      0     1    0     0  0   0   0   0  0     0   \n",
       "6    0    0   0    0   0    0      0     0    0     0  1   0   0   0  0     0   \n",
       "7    0    1   0    0   0    0      0     0    0     0  0   0   0   0  0     0   \n",
       "8    0    0   0    0   0    0      0     0    0     0  0   0   0   0  0     0   \n",
       "9    0    0   0    0   0    0      0     0    1     0  0   0   0   0  0     0   \n",
       "\n",
       "   amazon  cloud  at  google  \n",
       "0       0      0   0       0  \n",
       "1       0      0   0       0  \n",
       "2       0      0   0       0  \n",
       "3       0      0   0       0  \n",
       "4       0      0   0       0  \n",
       "5       0      0   0       0  \n",
       "6       0      0   0       0  \n",
       "7       0      0   0       0  \n",
       "8       0      0   0       0  \n",
       "9       0      0   0       0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_df.iloc[:10,:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further inhance my prediciton, I am going to remove words that occured more than 100 times which should remove stopwords like 'and' and 'for', which occurs almost in every headline and words that occured less than 5 times. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sum of each token occurrence \n",
    "word_counts = tokens_df.sum(axis = 0)\n",
    "\n",
    "# remove tokens fewer than 5 and more than 100\n",
    "tokens_df = tokens_df.loc[:, (word_counts >= 5) & (word_counts <= 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2800 entries, 0 to 2799\n",
      "Columns: 661 entries, as to nike\n",
      "dtypes: int64(661)\n",
      "memory usage: 14.1 MB\n"
     ]
    }
   ],
   "source": [
    "tokens_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we reduced tokens from 2310 to 661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will split the data into two sets: 80% training set and 20% test set, \n",
    "and I will use the train_test_split function from sckit-learn to do that \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tokens_df, articals['upvotes'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initializing the linear regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fitting the model\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# Calculating predictions on the test data\n",
    "predictions = lr.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean =  11.254356849122622\n",
      "Srandard Deviation =  27.118933312035175\n",
      "Mean Squared Error =  2358.5493277253954\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean = np.mean(predictions)\n",
    "std  = np.std(predictions)\n",
    "mse = mean_squared_error(predictions, y_test)\n",
    "\n",
    "print('Mean = ', mean)\n",
    "print('Srandard Deviation = ', std)\n",
    "print('Mean Squared Error = ', mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error is 2358 which is a big number. The mean  of upvotes is 11.25, and the standard deviation is 27.11.\n",
    "I will try a random forest model, maybe I can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean =  6.764285714285714\n",
      "Srandard deviation =  28.57224195270791\n",
      "Mean squared error =  2570.6464285714287\n"
     ]
    }
   ],
   "source": [
    "# initializing the the model\n",
    "rf = RandomForestClassifier(random_state = 1)\n",
    "\n",
    "#fitting the model\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "# Calculating predictions\n",
    "predictions = rf.predict(x_test)\n",
    "\n",
    "mean = np.mean(predictions)\n",
    "std  = np.std(predictions)\n",
    "\n",
    "mse = mean_squared_error(predictions, y_test)\n",
    "\n",
    "print('Mean = ', mean)\n",
    "print('Srandard deviation = ', std)\n",
    "print('Mean squared error = ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest mse is 2570.64 which is more than what I got from the regression model. \n",
    "I will leave it for now and try to do some more features engineering and hyperparameter optimization the future to get a better result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
